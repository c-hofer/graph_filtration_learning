{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import itertools\n",
    "import copy\n",
    "import uuid\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import chofer_torchex.pershom as pershom\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "\n",
    "from chofer_torchex import pershom\n",
    "ph = pershom.pershom_backend.__C.VertFiltCompCuda__vert_filt_persistence_batch\n",
    "\n",
    "from chofer_torchex.nn import SLayerRationalHat\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "import core.model\n",
    "from core.data import dataset_factory, train_test_split\n",
    "from core.utils import my_collate, evaluate\n",
    "from core.train_engine import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDB-MULTI', 'IMDB-BINARY', 'ENZYMES', 'PROTEINS', 'DD', 'NCI1']\n",
      "# Number of configurations 62\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\n",
    "        #'COLLAB',\n",
    "        'IMDB-MULTI',\n",
    "        'IMDB-BINARY',\n",
    "        'ENZYMES',\n",
    "        #'PTC_PGNN',\n",
    "        #'PTC_FM',\n",
    "        #'PTC_FR',\n",
    "        #'PTC_MM',\n",
    "        #'PTC_MR',\n",
    "        'PROTEINS',\n",
    "        'DD',\n",
    "        'NCI1',\n",
    "        #'MUTAG'\n",
    "]\n",
    "print(dataset_names)\n",
    "\n",
    "\n",
    "dataset_has_node_lab = {n: dataset_factory(n, verbose=False).num_node_lab is not None for n in dataset_names}\n",
    "dataset_has_node_lab\n",
    "\n",
    "training_cfg = {\n",
    "    'lr': 0.01, \n",
    "    'lr_drop_fact': 0.5, \n",
    "    'num_epochs': 60,\n",
    "    'epoch_step': 10,\n",
    "    'batch_size': 64,\n",
    "    'weight_decay': 10e-06,\n",
    "}\n",
    "training_cfgs = [training_cfg]\n",
    "\n",
    "model_cfg = {\n",
    "    'model_type': 'PershomModel',\n",
    "    'model_kwargs': {\n",
    "        'use_sup_lvlset_filt': False,\n",
    "        'filtration_kwargs': {\n",
    "            'use_node_deg': None,\n",
    "            'use_node_lab': None,\n",
    "            'num_gin': 1,\n",
    "            'hidden_dim': 64, \n",
    "            'use_mlp': None\n",
    "        }, \n",
    "        'classifier_kwargs': {\n",
    "            'num_struct_elem': 50\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_cfgs = []\n",
    "for a, b, c, d in itertools.product([True, False], [True, False], [True, False], [True, False],):\n",
    "    if (not a) and (not b):\n",
    "        continue\n",
    "\n",
    "    tmp = copy.deepcopy(model_cfg)\n",
    "    tmp['model_kwargs']['filtration_kwargs']['use_node_deg'] = a\n",
    "    tmp['model_kwargs']['filtration_kwargs']['use_node_lab'] = b\n",
    "    tmp['model_kwargs']['filtration_kwargs']['use_mlp'] = c\n",
    "    tmp['model_kwargs']['use_sup_lvlset_filt'] = d\n",
    "\n",
    "    model_cfgs.append(tmp)\n",
    "\n",
    "no_filt_learning = copy.deepcopy(model_cfg)\n",
    "no_filt_learning['model_kwargs']['filtration_kwargs'] = None\n",
    "\n",
    "model_cfgs.append(no_filt_learning)\n",
    "\n",
    "exp_cfgs = []\n",
    "for a, b, c in itertools.product(dataset_names, training_cfgs, model_cfgs):\n",
    "\n",
    "    # filter out datasets which have no node labels\n",
    "    if not dataset_has_node_lab[a]:\n",
    "        if c['model_kwargs']['filtration_kwargs'] is not None:\n",
    "            use_node_lab = c['model_kwargs']['filtration_kwargs']['use_node_lab']\n",
    "\n",
    "            if use_node_lab:\n",
    "                continue\n",
    "\n",
    "    tmp = {\n",
    "        'dataset_name': a, \n",
    "        'training': b, \n",
    "        'model': c\n",
    "    }\n",
    "    exp_cfgs.append(tmp)\n",
    "\n",
    "print(\"# Number of configurations\", len(exp_cfgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = '/home/pma/chofer/repositories/nips_2019_code/results'\n",
    "# with open(os.path.join(output_dir, 'exp_cfgs.json'), 'w') as fid:\n",
    "#     json.dump(exp_cfgs, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
