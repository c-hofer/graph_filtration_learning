{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate experiment configuration files\n",
    "\n",
    "The purpos of this notebook is to generate configurations files. Those files are the input to the \n",
    "train script (`train.py`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import itertools\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from core.data import dataset_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'REDDIT-BINARY': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which datasets have nodelabels attached. This may take a while. \n",
    "dataset_names = [\n",
    "        'REDDIT-BINARY',\n",
    "#         'REDDIT-MULTI-5K',\n",
    "#         'COLLAB',\n",
    "        #'IMDB-MULTI',\n",
    "        #'IMDB-BINARY',\n",
    "         #'ENZYMES',\n",
    "         #'PTC_PGNN',\n",
    "         #'PTC_FM',\n",
    "         #'PTC_FR',\n",
    "         #'PTC_MM',\n",
    "         #'PTC_MR',\n",
    "         #'PROTEINS',\n",
    "         #'DD',\n",
    "         #'NCI1',\n",
    "         #'MUTAG'\n",
    "]\n",
    "\n",
    "dataset_has_node_lab = {n: dataset_factory(n, verbose=False).num_node_lab is not None for n in dataset_names}\n",
    "dataset_has_node_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization related part of the configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = {\n",
    "    'lr': 0.01, \n",
    "    'lr_drop_fact': 0.5, \n",
    "    'num_epochs': 100,\n",
    "    'epoch_step': 20,\n",
    "    'batch_size': 32,\n",
    "    'weight_decay': 10e-06,\n",
    "    'validation_ratio': 0.1\n",
    "}\n",
    "training_cfgs = [training_cfg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model related part of the configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pershom rigid filtration ...\n",
    "proto = {\n",
    "    'model_type': 'PershomRigidDegreeFilt',\n",
    "    'use_super_level_set_filtration': None, \n",
    "    'num_struct_elements': 100, \n",
    "    'cls_hidden_dimension': 64, \n",
    "    'drop_out': 0.0\n",
    "}\n",
    "model_cfgs_PershomRigidDegreeFilt = []\n",
    "for b in [False, True]:\n",
    "    tmp = copy.deepcopy(proto)\n",
    "    \n",
    "    tmp['use_super_level_set_filtration'] = b\n",
    "    \n",
    "    model_cfgs_PershomRigidDegreeFilt.append(tmp)\n",
    "    \n",
    "len(model_cfgs_PershomRigidDegreeFilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pershom learnt filtration ...\n",
    "proto = {\n",
    "    'model_type': 'PershomLearnedFilt',\n",
    "    'use_super_level_set_filtration': None, \n",
    "    'use_node_degree': None, \n",
    "    'set_node_degree_uninformative': True, \n",
    "    'use_node_label': None, \n",
    "    'gin_number': 1, \n",
    "    'gin_dimension': 64,\n",
    "    'gin_mlp_type': 'lin_bn_lrelu_lin', \n",
    "    'num_struct_elements': 100, \n",
    "    'cls_hidden_dimension': 64, \n",
    "    'drop_out': 0.0   \n",
    "}\n",
    "model_cfgs_PershomLearnedFilt = []\n",
    "\n",
    "B = [(True, True), (False, True), (True, False)]\n",
    "\n",
    "for (a, b), c, d, e in itertools.product(B, [True], [64], [1]):\n",
    "    tmp = copy.deepcopy(proto)\n",
    "\n",
    "    tmp['use_node_degree'] = a\n",
    "    tmp['use_node_label']  = b\n",
    "    tmp['use_super_level_set_filtration'] = c    \n",
    "\n",
    "    tmp['gin_dimension'] = d\n",
    "    tmp['gin_number'] = e\n",
    "\n",
    "    model_cfgs_PershomLearnedFilt.append(tmp)\n",
    "    \n",
    "len(model_cfgs_PershomLearnedFilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GIN ... \n",
    "proto = {\n",
    "    'model_type': 'GIN',\n",
    "    'use_node_degree': None, \n",
    "    'use_node_label': None, \n",
    "    'gin_number': None, \n",
    "    'gin_dimension': 64,\n",
    "    'gin_mlp_type': 'lin_bn_lrelu_lin', \n",
    "    'cls_hidden_dimension': 64, \n",
    "    'set_node_degree_uninformative': None,\n",
    "    'pooling_strategy': 'sort',\n",
    "    'drop_out': 0.5 \n",
    "}\n",
    "model_cfgs_GIN = []\n",
    "\n",
    "B = [(True, True), (False, True), (True, False)]\n",
    "\n",
    "for (a, b), c, d in itertools.product(B, [1], [True]):\n",
    "    tmp = copy.deepcopy(proto)\n",
    "\n",
    "    tmp['use_node_degree'] = a\n",
    "    tmp['use_node_label'] = b\n",
    "    tmp['gin_number'] = c\n",
    "    tmp['set_node_degree_uninformative'] = d\n",
    "\n",
    "    model_cfgs_GIN.append(tmp)\n",
    "    \n",
    "len(model_cfgs_GIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleNNBaseline ... \n",
    "proto = {\n",
    "    'model_type': 'SimpleNNBaseline',\n",
    "    'use_node_degree': None, \n",
    "    'use_node_label': None, \n",
    "    'gin_dimension': 64,\n",
    "    'gin_mlp_type': 'lin_bn_lrelu_lin', \n",
    "    'cls_hidden_dimension': 64, \n",
    "    'set_node_degree_uninformative': None,\n",
    "    'pooling_strategy': 'sum',\n",
    "    'drop_out': None \n",
    "}\n",
    "model_cfgs_SimpleNNBaseline = []\n",
    "\n",
    "B = [(True, True), (False, True), (True, False)]\n",
    "\n",
    "for (a, b), c, d in itertools.product(B, [False], [0.0, 0.5]):\n",
    "    tmp = copy.deepcopy(proto)\n",
    "\n",
    "    tmp['use_node_degree'] = a\n",
    "    tmp['use_node_label'] = b\n",
    "    tmp['set_node_degree_uninformative'] = c\n",
    "    tmp['drop_out'] = d\n",
    "\n",
    "    model_cfgs_SimpleNNBaseline.append(tmp)\n",
    "    \n",
    "len(model_cfgs_SimpleNNBaseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we combine those parts and write the cfg file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(dataset_names, training_cfgs, model_cfgs, tag=\"\"):\n",
    "    exp_cfgs = []\n",
    "    continued = 0\n",
    "    for a, b, c in itertools.product(dataset_names, training_cfgs, model_cfgs):\n",
    "\n",
    "        # filter out datasets which have no node labels\n",
    "        ds_has_node_lab = dataset_has_node_lab[a]\n",
    "\n",
    "        if 'use_node_label' in c:\n",
    "            use_node_lab = c['use_node_label']\n",
    "\n",
    "            if (not ds_has_node_lab) and use_node_lab:\n",
    "#                 print(a, c['model_type'])\n",
    "                continue\n",
    "\n",
    "        tmp = {\n",
    "            'dataset_name': a, \n",
    "            'training': b, \n",
    "            'model': c, \n",
    "            'tag': tag\n",
    "        }\n",
    "        exp_cfgs.append(tmp)\n",
    "        \n",
    "    return exp_cfgs\n",
    "\n",
    "def write_file(dataset_names, training_cfgs, model_cfgs, output_dir, tag=\"\", file_name=None):\n",
    "    exp_cfgs = combine(dataset_names, training_cfgs, model_cfgs, tag=tag)\n",
    "    if file_name is None:\n",
    "        file_name = \"exp_cfgs__\" + \"_\".join(dataset_names) + \".json\"\n",
    "        \n",
    "    with open(file_name, 'w') as fid:\n",
    "        json.dump(exp_cfgs, fid)\n",
    "        \n",
    "    print('Num cfgs: ', len(exp_cfgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cfgs:  1\n"
     ]
    }
   ],
   "source": [
    "# Write cfg file for, e.g., learned filtration setup...\n",
    "write_file(dataset_names, \n",
    "           training_cfgs, \n",
    "           model_cfgs_PershomLearnedFilt, \n",
    "           output_dir, \n",
    "           file_name='my_config.json', \n",
    "           tag=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
